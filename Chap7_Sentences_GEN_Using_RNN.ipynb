{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq8FYWBFu8FgP1fiPMYV3c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauvatu/HS-algorithm/blob/master/Chap7_Sentences_GEN_Using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of Sentence Generation**\n",
        "\n",
        "rnnlm_gen\n",
        "\n"
      ],
      "metadata": {
        "id": "TA9-rhXnVZ-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MgiJKA9SUxLC"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.functions import softmax\n",
        "from ch06.rnnlm import Rnnlm\n",
        "from ch06.better_rnnlm import BetterRnnlm\n",
        "\n",
        "\n",
        "class RnnlmGen(Rnnlm):\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]\n",
        "\n",
        "        x = start_id\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)\n",
        "            score = self.predict(x)\n",
        "            p = softmax(score.flatten())\n",
        "\n",
        "            sampled = np.random.choice(len(p), size=1, p=p)\n",
        "            if (skip_ids is None) or (sampled not in skip_ids):\n",
        "                x = sampled\n",
        "                word_ids.append(int(x))\n",
        "\n",
        "        return word_ids\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.lstm_layer.h, self.lstm_layer.c\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.lstm_layer.set_state(*state)\n",
        "\n",
        "\n",
        "class BetterRnnlmGen(BetterRnnlm):\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]\n",
        "\n",
        "        x = start_id\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)\n",
        "            score = self.predict(x).flatten()\n",
        "            p = softmax(score).flatten()\n",
        "\n",
        "            sampled = np.random.choice(len(p), size=1, p=p)\n",
        "            if (skip_ids is None) or (sampled not in skip_ids):\n",
        "                x = sampled\n",
        "                word_ids.append(int(x))\n",
        "\n",
        "        return word_ids\n",
        "\n",
        "    def get_state(self):\n",
        "        states = []\n",
        "        for layer in self.lstm_layers:\n",
        "            states.append((layer.h, layer.c))\n",
        "        return states\n",
        "\n",
        "    def set_state(self, states):\n",
        "        for layer, state in zip(self.lstm_layers, states):\n",
        "            layer.set_state(*state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of Sentence Generation**\n",
        "\n",
        "Generate_text\n",
        "\n"
      ],
      "metadata": {
        "id": "hfeZSzTGq7g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from rnnlm_gen import RnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = RnnlmGen()\n",
        "model.load_params('ch06/Rnnlm.pkl')\n",
        "\n",
        "#Setting start and skip characters\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "#Sentence generation\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iiLhe0bXDqB",
        "outputId": "21463516-5444-4003-80d1-babb2771f8f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you need to damn work for persistent.\n",
            " he has plenty of figure we had it clear that i can just n't be able to pay he says.\n",
            " note what 's important analysts reason bonds from brazilian presence to gary and california beach news data mostly division of united producers executive last spring.\n",
            " if you think we consultants of the industry authority saying it is likely to bolster the japanese management and can the problem.\n",
            " mr. roman has bill these partners mr. gorbachev design that ban an notification political legal position will retire if he had for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from dataset import sequence\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    sequence.load_data('dataset/addition.txt', seed=1984)\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)\n",
        "# (45000, 7) (45000, 5)\n",
        "# (5000, 7) (5000, 5)\n",
        "\n",
        "print(x_train[0])\n",
        "print(t_train[0])\n",
        "# [ 3  0  2  0  0 11  5]\n",
        "# [ 6  0 11  7  5]\n",
        "\n",
        "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
        "print(''.join([id_to_char[c] for c in t_train[0]]))\n",
        "# 71+118\n",
        "# _189"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "x5BDYa1saF_L",
        "outputId": "c7bb50cf-324f-45b0-a35a-f3dc11106714"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No file: dataset/addition.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ba3190d70a33>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/addition.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1984\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchar_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate_better_text\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from rnnlm_gen import BetterRnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "\n",
        "model = BetterRnnlmGen()\n",
        "model.load_params('ch06/BetterRnnlm.pkl')\n",
        "\n",
        "#Setting start and skip characters\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "#Sentence generation\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "\n",
        "print(txt)\n",
        "\n",
        "\n",
        "model.reset_state()\n",
        "\n",
        "start_words = 'the meaning of life is'\n",
        "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
        "\n",
        "for x in start_ids[:-1]:\n",
        "    x = np.array(x).reshape(1, 1)\n",
        "    model.predict(x)\n",
        "\n",
        "word_ids = model.generate(start_ids[-1], skip_ids)\n",
        "word_ids = start_ids[:-1] + word_ids\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "print('-' * 50)\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKY7gMJEmMy5",
        "outputId": "78f6c3a9-55f9-47ae-8454-33af25bd2fd6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you were discussing problem says robert a staff of the sales service at wang and in turn this month.\n",
            " and the great way within about to recruit steel operators have as many as many as used to operate maintained mr. levy.\n",
            " david take a job to bank to acquire wells fargo inc. 's navy and ongoing health tools.\n",
            " he is a waste buyer peddling back to collectors in america.\n",
            " instead he refused and nor today the offer is signed end the strategy on a multimillion-dollar basis it may be superior to catastrophes to commodity publishing church\n",
            "--------------------------------------------------\n",
            "the meaning of life is out on major terms on the job but government officials are that this is going to be cautious for computers in our areas he says.\n",
            " the solution is that the chain 's industry characters will destroy distributing units those of their new products and materials it can be and go the state for abortions says b. terry clark of the reagan administration committee for in washington.\n",
            " so far the national semiconductor institute on the quarter of comparison with carbon dioxide introduced by the black club 's response has become high priority in any kind of relief not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of seq2seq**\n",
        "\n",
        "*   Decoder Class\n",
        "*   Encoder Class\n",
        "\n"
      ],
      "metadata": {
        "id": "5z3BhH72nqeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "\n",
        "        self.params = self.embed.params + self.lstm.params\n",
        "        self.grads = self.embed.grads + self.lstm.grads\n",
        "        self.hs = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        self.hs = hs\n",
        "        return hs[:, -1, :]\n",
        "\n",
        "    def backward(self, dh):\n",
        "        dhs = np.zeros_like(self.hs)\n",
        "        dhs[:, -1, :] = dh\n",
        "\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n",
        "\n",
        "\n",
        "class Decoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        out = self.lstm.forward(out)\n",
        "        score = self.affine.forward(out)\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dout = self.embed.backward(dout)\n",
        "        dh = self.lstm.dh\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array(sample_id).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "            out = self.lstm.forward(out)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(int(sample_id))\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class Seq2seq(BaseModel):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = Decoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
        "\n",
        "        h = self.encoder.forward(xs)\n",
        "        score = self.decoder.forward(decoder_xs, h)\n",
        "        loss = self.softmax.forward(score, decoder_ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.softmax.backward(dout)\n",
        "        dh = self.decoder.backward(dout)\n",
        "        dout = self.encoder.backward(dh)\n",
        "        return dout\n",
        "\n",
        "    def generate(self, xs, start_id, sample_size):\n",
        "        h = self.encoder.forward(xs)\n",
        "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
        "        return sampled"
      ],
      "metadata": {
        "id": "aVTmXke4ncfu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from seq2seq import Seq2seq\n",
        "from peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "\n",
        "#Loading the dataset\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# Reverse input? =================================================\n",
        "is_reverse = False  # True\n",
        "if is_reverse:\n",
        "    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "# ================================================================\n",
        "\n",
        "#Setting hyperparameters\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "max_epoch = 25\n",
        "max_grad = 5.0\n",
        "\n",
        "# Normal or Peeky? ==============================================\n",
        "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# ================================================================\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "\n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse)\n",
        "\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    acc_list.append(acc)\n",
        "    print('val acc %.3f%%' % (acc * 100))\n",
        "\n",
        "#drawing a graph\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "wcZaVzSsoPmW",
        "outputId": "b66dc951-278f-492a-f594-95bf008884ff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-38382866982f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# データセットの読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'addition.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mchar_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset/sequence.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_name, seed)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# 넘파이 배열 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
          ]
        }
      ]
    }
  ]
}