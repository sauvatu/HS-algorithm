{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2ePMl6/T5ASmpv8wQ3eWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauvatu/HS-algorithm/blob/master/Chp4_Word2Vec_Improvement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word2Vec Improvement1**\n",
        "\n",
        "If extracting a specific row, specify the desired row."
      ],
      "metadata": {
        "id": "mONy81kW_lBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64d9Knlg_SHa",
        "outputId": "8dfe26d9-411e-4870-be33-a6daf2906f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]\n",
            " [15 16 17]\n",
            " [18 19 20]]\n",
            "Printing content from index 2:  [6 7 8]\n",
            "Printing content from index 5:  [15 16 17]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4,  5],\n",
              "       [ 0,  1,  2],\n",
              "       [ 9, 10, 11],\n",
              "       [ 0,  1,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "print(W)\n",
        "\n",
        "# Get second row\n",
        "# -> index=2 Word vector corresponding to\n",
        "print('Printing content from index 2: ', + W[2])\n",
        "#Printing index 5\n",
        "print('Printing content from index 5: ', + W[5])\n",
        "\n",
        "# Multiple rows from weight W\n",
        "# Example of extracting all at once\n",
        "idx = np.array([1, 0, 3, 0])\n",
        "W[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of Embedding Layer**"
      ],
      "metadata": {
        "id": "VJA923fwCrJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializes the embedding layer with a single parameter W, which represents the embedding matrix.\n",
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "#Retrieves the embedding vectors corresponding to the given indices from the embedding matrix W.\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "#Updates dW by accumulating gradients based on the indices idx and the corresponding gradients dout.\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "        np.add.at(dW, self.idx, dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "agUhxwYhCsZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation Embedding Dot Layer"
      ],
      "metadata": {
        "id": "KsL0HPiWKu5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It initializes an instance of the Embedding class with the provided embedding matrix W.\n",
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grads\n",
        "        self.cache = None\n",
        "\n",
        "#Retrieves the embeddings corresponding to the indices using the Embedding layer.\n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forward(idx)\n",
        "        out = np.sum(target_W * h, axis=1)\n",
        "\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "#Computes gradients for the target embeddings (target_W) by multiplying dout with h.\n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh"
      ],
      "metadata": {
        "id": "JWzdhi_rK2Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of Sampling based on probability distribution."
      ],
      "metadata": {
        "id": "S6chxbJgMM1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling example based on probability distribution\n",
        "import numpy as np\n",
        "\n",
        "# Randomly samples one of the numbers 0 to 9\n",
        "print(np.random.choice(10))\n",
        "print(np.random.choice(10))\n",
        "\n",
        "# Randomly sample only one from words\n",
        "words = ['you', 'say', 'goodbye', 'i', 'hello', '.']\n",
        "print(np.random.choice(words))\n",
        "\n",
        "print(np.random.choice(words, size=5))\n",
        "# Randomly sampled only (no duplicates)\n",
        "print(np.random.choice(words, size=5, replace=False))\n",
        "\n",
        "# Sampling according to probability distribution\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "print(np.random.choice(words, p=p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvG0TtFxLGSu",
        "outputId": "4b3e63b3-2c6d-42fc-d14a-d6f3d96fbed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "8\n",
            "i\n",
            "['you' 'i' 'i' 'you' 'hello']\n",
            "['you' 'goodbye' 'i' 'hello' '.']\n",
            "say\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In sampling, apply 0.75 squared\n",
        "to values of the probability distribution.\n",
        "\n",
        "*   In order not to throw away words that have a low probability of appearing\n",
        "\n"
      ],
      "metadata": {
        "id": "AYs_lkkVOXxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "\n",
        "new_p /= np.sum(new_p)\n",
        "print(new_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl988pXkOv2z",
        "outputId": "69b0ec11-4c99-43ea-8ab3-ae473ba7f8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.64196878 0.33150408 0.02652714]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Negative Sampling Implementation**"
      ],
      "metadata": {
        "id": "46qO22K0S8Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "import collections\n",
        "\n",
        "# Negative Sampling class implementation\n",
        "# chap04/negative_sampling_layer.py\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:  # == CPU\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0  #To prevent it from being pulled out\n",
        "                p /= p.sum()  # Normalize again\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size,\n",
        "                                                         size=self.sample_size,\n",
        "                                                         replace=False, p=p)\n",
        "\n",
        "        else:\n",
        "            # GPU(cupy)When calculating, speed takes priority.\n",
        "            # Negative examples may include targets.\n",
        "            negative_sample = np.random.choice(self.vocab_size,\n",
        "                                               size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample\n",
        "\n"
      ],
      "metadata": {
        "id": "cyJ8nbWTSsjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1, 3, 0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVoqqiHKa7Lm",
        "outputId": "6aed72d6-fe8e-42f8-8db8-ee3fe1478160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 3]\n",
            " [1 2]\n",
            " [2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "import collections\n",
        "# chap04/negative_sampling_layer.py\n",
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "#Creates a UnigramSampler instance for negative sampling, which will be used to\n",
        "#generate negative samples from the corpus.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "#Computes the loss for the positive target using the dot product of h and the target embedding,\n",
        "#and calculates the loss using SigmoidWithLoss.\n",
        "        # Positive Yes Net Spread\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # Negative Yes Net Spread\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]  #It seems to mean that it is a target corresponding to embed_dot\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "#Backpropagates gradients through each loss and embedding dot product layer.\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh"
      ],
      "metadata": {
        "id": "n1B53zuubHNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3 **word2vec** (Improved Version) **Learning**\n",
        "*   4.3.1 Implementation of CBOW Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r4FksapDcMqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chap04/cbow.py\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import Embedding\n",
        "from negative_sampling_layer import NegativeSamplingLoss\n",
        "\n",
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # weight initialization\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "        # Create Layer\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embedding use layers\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # Gather all weights and gradients into an array.\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # Store the distributed representation of the word in an instance variable.\n",
        "        self.word_vecs1 = W_in\n",
        "        self.word_vecs2 = W_out\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)  # average\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "JVhWuHnscmYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CBOW** Training Code"
      ],
      "metadata": {
        "id": "vUfuHWBuewH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chap04/train.py\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common import config\n",
        "# Uncomment below to run on GPU (requires CuPy).\n",
        "# ===============================================\n",
        "config.GPU = True\n",
        "# ===============================================\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from cbow import CBOW\n",
        "#from skip_gram import SkipGram\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb\n",
        "\n",
        "# Hyperparameter settings\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# read data\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "# Create models, etc.\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# Start learning\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "pass"
      ],
      "metadata": {
        "id": "jE_eoMQFlj5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import pickle\n",
        "from common.util import most_similar, analogy\n",
        "pkl_file = './cbow_params.pkl'\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "word_vecs = params['word_vecs']\n",
        "word_to_id = params['word_to_id']\n",
        "id_to_word = params['id_to_word']\n",
        "# Pick the most similar word\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSUCQCNKSJS1",
        "outputId": "6de93eaf-456f-4b07-8e9c-19695682463f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 유추(analogy) 작업\n",
        "print('-'*50)\n",
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHICqh-ASRDF",
        "outputId": "227df183-4626-44ce-9b2c-21ba6f2fecff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.16015625\n",
            " veto: 4.9296875\n",
            " ounce: 4.69140625\n",
            " earthquake: 4.6328125\n",
            " successor: 4.609375\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.55078125\n",
            " points: 4.25\n",
            " began: 4.09375\n",
            " comes: 3.98046875\n",
            " oct.: 3.90625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 5.21875\n",
            " average: 4.7265625\n",
            " yield: 4.20703125\n",
            " cattle: 4.1875\n",
            " priced: 4.1796875\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " more: 6.6484375\n",
            " less: 6.0625\n",
            " rather: 5.21875\n",
            " slower: 4.734375\n",
            " greater: 4.671875\n"
          ]
        }
      ]
    }
  ]
}